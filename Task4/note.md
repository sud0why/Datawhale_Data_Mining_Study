# å»ºæ¨¡è°ƒå‚

## ä¸€ã€æ¦‚è¿°

### 1.1. çº¿æ€§å›å½’æ¨¡å‹

#### çº¿æ€§å›å½’å¯¹äºç‰¹å¾çš„è¦æ±‚

* ç‰¹å¾ç¬¦åˆçº¿æ€§è¡Œå’Œå¯åŠ æ€§ã€‚å‡è®¾å› å˜é‡ä¸ºYï¼Œè‡ªå˜é‡ä¸ºX1ï¼ŒX2ï¼Œåˆ™å›å½’åˆ†æçš„é»˜è®¤å‡è®¾ä¸ºY=b+a1X1+a2X2+Îµã€‚
    çº¿æ€§æ€§ï¼šX1æ¯å˜åŠ¨ä¸€ä¸ªå•ä½ï¼ŒYç›¸åº”å˜åŠ¨a1ä¸ªå•ä½ï¼Œä¸X1çš„ç»å¯¹æ•°å€¼å¤§å°æ— å…³ã€‚å¯åŠ æ€§ï¼šX1å¯¹Yçš„å½±å“æ˜¯ç‹¬ç«‹äºå…¶ä»–è‡ªå˜é‡ï¼ˆå¦‚X2ï¼‰çš„ã€‚
* ç‰¹å¾ä¹‹é—´åº”ç›¸äº’ç‹¬ç«‹ã€‚

#### å¤„ç†é•¿å°¾åˆ†å¸ƒ

#### ç†è§£çº¿æ€§å›å½’æ¨¡å‹

* çº¿æ€§å›å½’äº”å¤§å‡è®¾ï¼š1ï¼‰çº¿æ€§æ€§ & å¯åŠ æ€§ï¼Œ2ï¼‰è¯¯å·®é¡¹ï¼ˆÎµï¼‰ä¹‹é—´åº”ç›¸äº’ç‹¬ç«‹ï¼Œ3ï¼‰è‡ªå˜é‡ï¼ˆX1ï¼ŒX2ï¼‰ä¹‹é—´åº”ç›¸äº’ç‹¬ç«‹ï¼Œ4ï¼‰è¯¯å·®é¡¹ï¼ˆÎµï¼‰çš„æ–¹å·®åº”ä¸ºå¸¸æ•°ï¼Œ5ï¼‰è¯¯å·®é¡¹ï¼ˆÎµï¼‰åº”å‘ˆæ­£æ€åˆ†å¸ƒ
* çº¿æ€§å›å½’æœ€æ™®é€šçš„å½¢å¼æ˜¯
![equation1](images/equation1.svg)

    å…¶ä¸­xå‘é‡ä»£è¡¨ä¸€æ¡æ ·æœ¬{x1,x2,x3....xn}ï¼Œå…¶ä¸­x1ï¼Œx2ï¼Œx3ä»£è¡¨æ ·æœ¬çš„å„ä¸ªç‰¹å¾ï¼Œwæ˜¯ä¸€æ¡å‘é‡ä»£è¡¨äº†æ¯ä¸ªç‰¹å¾æ‰€å çš„æƒé‡ï¼Œbæ˜¯ä¸€ä¸ªæ ‡é‡ä»£è¡¨ç‰¹å¾éƒ½ä¸º0æ—¶çš„é¢„æµ‹å€¼ï¼Œå¯ä»¥è§†ä¸ºæ¨¡å‹çš„basisæˆ–è€…biasã€‚
* æŸå¤±å‡½æ•°ã€‚ä¸€ä¸ªå­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºäº†ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹ã€ç­–ç•¥ã€ç®—æ³•ï¼Œä¸ºäº†è·å¾—wå’Œbä¸€èˆ¬é‡‡å–ä»¥ä¸‹ç­–ç•¥ï¼šå‡å°‘åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å€¼f(x)ä¸çœŸå®å€¼yçš„å·®åˆ«ï¼Œä»è€Œè·å¾—ä¸€ä¸ªæœ€ä½³çš„æƒé‡å‚æ•°ï¼Œå› æ­¤è¿™é‡Œé‡‡ç”¨æœ€å°äºŒä¹˜ä¼°è®¡ã€‚

![equation1](images/equation2.svg)

* ä¼˜åŒ–æ–¹æ³•ã€‚

    æœ€å°äºŒä¹˜ä¼˜åŒ–çš„æ€è·¯æ˜¯çº¿æ€§ä»£æ•°ä¸­çš„çŸ©é˜µæ±‚å¯¼ï¼Œè®©losså–åˆ°æœ€å°ï¼Œåªéœ€è¦å¯¹è¿™ä¸ªå¼å­è¿›è¡Œæ±‚å¯¼ï¼Œå¯¼æ•°ä¸º0çš„åœ°æ–¹å°±æ˜¯æå€¼ç‚¹ã€‚

![equation1](images/equation3.svg)

    æ¢¯åº¦ä¸‹é™çš„ç­–ç•¥æ˜¯ä¸€æ­¥ä¸€æ­¥çš„å¾€è®©losså˜åˆ°æœ€å°å€¼çš„æ–¹å‘èµ°ï¼Œç›´åˆ°èµ°åˆ°é‚£ä¸ªç‚¹ã€‚

![equation1](images/equation4.svg)

![equation1](images/equation5.svg)

### 1.2. æ¨¡å‹æ€§èƒ½éªŒè¯

#### è¯„ä»·å‡½æ•°ä¸ç›®æ ‡å‡½æ•°

* ç›®æ ‡å‡½æ•°æ˜¯æœ€ç»ˆéœ€è¦ä¼˜åŒ–çš„å‡½æ•°(obj=loss+Î©)ï¼Œå…¶ä¸­åŒ…æ‹¬ç»éªŒæŸå¤±å’Œç»“æ„æŸå¤±ã€‚ç»éªŒæŸå¤±(loss)å°±æ˜¯æŸå¤±å‡½æ•°/ä»£ä»·å‡½æ•°ã€‚ç»“æ„æŸå¤±(Î©)å°±æ˜¯æ­£åˆ™é¡¹ä¹‹ç±»çš„æ¥æ§åˆ¶æ¨¡å‹å¤æ‚ç¨‹åº¦çš„å‡½æ•°ã€‚
* è¯„ä»·å‡½æ•°å³æŸå¤±å‡½æ•°/ä»£ä»·å‡½æ•°ï¼Œç”¨äºè¯„ä¼°é¢„æµ‹å€¼å’ŒçœŸå®å€¼å·®å¼‚ã€‚å¸¸ç”¨æœ‰ï¼šå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Errorï¼ŒMAEï¼‰ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼ŒMSEï¼‰ï¼Œå¹³å‡ç»å¯¹ç™¾åˆ†è¯¯å·®ï¼ˆMean Absolute Percentage Errorï¼ŒMAPEï¼‰ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRoot Mean Squared Errorï¼‰ï¼Œ R2ï¼ˆR-Squareï¼‰ã€‚

#### äº¤å‰éªŒè¯æ–¹æ³•

 * KæŠ˜äº¤å‰éªŒè¯ï¼Œåˆå§‹é‡‡æ ·åˆ†å‰²æˆKä¸ªå­æ ·æœ¬ï¼Œä¸€ä¸ªå•ç‹¬çš„å­æ ·æœ¬è¢«ä¿ç•™ä½œä¸ºéªŒè¯æ¨¡å‹çš„æ•°æ®ï¼Œå…¶ä»–K-1ä¸ªæ ·æœ¬ç”¨æ¥è®­ç»ƒã€‚äº¤å‰éªŒè¯é‡å¤Kæ¬¡ï¼Œæ¯ä¸ªå­æ ·æœ¬éªŒè¯ä¸€æ¬¡ï¼Œå¹³å‡Kæ¬¡çš„ç»“æœæˆ–è€…ä½¿ç”¨å…¶å®ƒç»“åˆæ–¹å¼ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå•ä¸€ä¼°æµ‹ã€‚è¿™ä¸ªæ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºï¼ŒåŒæ—¶é‡å¤è¿ç”¨éšæœºäº§ç”Ÿçš„å­æ ·æœ¬è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œæ¯æ¬¡çš„ç»“æœéªŒè¯ä¸€æ¬¡ï¼Œ10æŠ˜äº¤å‰éªŒè¯æ˜¯æœ€å¸¸ç”¨çš„ã€‚

#### ç•™ä¸€éªŒè¯æ–¹æ³•

* åªä½¿ç”¨åŸæœ¬æ ·æœ¬ä¸­çš„ä¸€é¡¹æ¥å½“åšéªŒè¯èµ„æ–™ï¼Œ è€Œå‰©ä½™çš„åˆ™ç•™ä¸‹æ¥å½“åšè®­ç»ƒèµ„æ–™ã€‚ è¿™ä¸ªæ­¥éª¤ä¸€ç›´æŒç»­åˆ°æ¯ä¸ªæ ·æœ¬éƒ½è¢«å½“åšä¸€æ¬¡éªŒè¯èµ„æ–™ã€‚ äº‹å®ä¸Šï¼Œè¿™ç­‰åŒäºå’ŒK-fold äº¤å‰éªŒè¯æ˜¯ä¸€æ ·çš„ï¼Œå…¶ä¸­Kä¸ºåŸæœ¬æ ·æœ¬ä¸ªæ•°ã€‚ 

#### é’ˆå¯¹æ—¶é—´åºåˆ—é—®é¢˜çš„éªŒè¯

* åœ¨æŸäº›ä¸æ—¶é—´ç›¸å…³çš„æ•°æ®é›†ä¸Šï¼Œåªèƒ½é€šè¿‡å‰é¢çš„æ•°æ®é¢„æµ‹åé¢çš„ç»“æœï¼Œä½¿ç”¨äº¤å‰éªŒè¯åè€Œåæ˜ äº†ä¸çœŸå®çš„æƒ…å†µï¼Œå¦‚é€šè¿‡2018å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹2017å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼ã€‚å› æ­¤éœ€è¦é‡‡ç”¨æ—¶é—´é¡ºåºå¯¹æ•°æ®é›†è¿›è¡Œåˆ†éš”ï¼Œé€‰ç”¨é å‰æ—¶é—´çš„(k-1)/kæ ·æœ¬å½“ä½œè®­ç»ƒé›†ï¼Œé åæ—¶é—´çš„1/kå½“ä½œéªŒè¯é›†

#### å­¦ä¹ ç‡æ›²çº¿ä½œç”¨

* ä¸€ç§ç”¨æ¥åˆ¤æ–­è®­ç»ƒæ¨¡å‹çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è§‚å¯Ÿç»˜åˆ¶å‡ºæ¥çš„å­¦ä¹ æ›²çº¿å›¾ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒç›´è§‚çš„äº†è§£åˆ°æˆ‘ä»¬çš„æ¨¡å‹å¤„äºä¸€ä¸ªä»€ä¹ˆæ ·çš„çŠ¶æ€ï¼Œå¦‚ï¼šè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰æˆ–æ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰
![learning_curve](images/learning_curve.png)
1) è§‚å¯Ÿå·¦ä¸Šå›¾ï¼Œè®­ç»ƒé›†å‡†ç¡®ç‡ä¸éªŒè¯é›†å‡†ç¡®ç‡æ”¶æ•›ï¼Œä½†æ˜¯ä¸¤è€…æ”¶æ•›åçš„å‡†ç¡®ç‡è¿œå°äºæˆ‘ä»¬çš„æœŸæœ›å‡†ç¡®ç‡ï¼ˆä¸Šé¢é‚£æ¡çº¢çº¿ï¼‰ï¼Œæ‰€ä»¥ç”±å›¾å¯å¾—è¯¥æ¨¡å‹å±äºæ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰é—®é¢˜ã€‚ç”±äºæ¬ æ‹Ÿåˆï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¢åŠ æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæ¯”å¦‚ï¼Œå¢åŠ ç‰¹å¾ã€å¢åŠ æ ‘çš„æ·±åº¦ã€å‡å°æ­£åˆ™é¡¹ç­‰ç­‰ï¼Œæ­¤æ—¶å†å¢åŠ æ•°æ®é‡æ˜¯ä¸èµ·ä½œç”¨çš„ã€‚
2) è§‚å¯Ÿå³ä¸Šå›¾ï¼Œè®­ç»ƒé›†å‡†ç¡®ç‡é«˜äºæœŸæœ›å€¼ï¼ŒéªŒè¯é›†åˆ™ä½äºæœŸæœ›å€¼ï¼Œä¸¤è€…ä¹‹é—´æœ‰å¾ˆå¤§çš„é—´è·ï¼Œè¯¯å·®å¾ˆå¤§ï¼Œå¯¹äºæ–°çš„æ•°æ®é›†æ¨¡å‹é€‚åº”æ€§è¾ƒå·®ï¼Œæ‰€ä»¥ç”±å›¾å¯å¾—è¯¥æ¨¡å‹å±äºè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰é—®é¢˜ã€‚ç”±äºè¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥æˆ‘ä»¬é™ä½æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæ¯”å¦‚å‡å°æ ‘çš„æ·±åº¦ã€å¢å¤§åˆ†è£‚èŠ‚ç‚¹æ ·æœ¬æ•°ã€å¢å¤§æ ·æœ¬æ•°ã€å‡å°‘ç‰¹å¾æ•°ç­‰ç­‰ã€‚
3) ä¸€ä¸ªæ¯”è¾ƒç†æƒ³çš„å­¦ä¹ æ›²çº¿å›¾åº”å½“æ˜¯ï¼šä½åå·®ã€ä½æ–¹å·®ï¼Œå³æ”¶æ•›ä¸”è¯¯å·®å°ã€‚

#### éªŒè¯æ›²çº¿çš„ä½œç”¨

* éªŒè¯æ›²çº¿å’Œå­¦ä¹ æ›²çº¿çš„åŒºåˆ«æ˜¯ï¼Œæ¨ªè½´ä¸ºæŸä¸ªè¶…å‚æ•°çš„ä¸€ç³»åˆ—å€¼ï¼Œç”±æ­¤æ¥çœ‹ä¸åŒå‚æ•°è®¾ç½®ä¸‹æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œè€Œä¸æ˜¯ä¸åŒè®­ç»ƒé›†å¤§å°ä¸‹çš„å‡†ç¡®ç‡ã€‚
    ä»éªŒè¯æ›²çº¿ä¸Šå¯ä»¥çœ‹åˆ°éšç€è¶…å‚æ•°è®¾ç½®çš„æ”¹å˜ï¼Œæ¨¡å‹å¯èƒ½ä»æ¬ æ‹Ÿåˆåˆ°åˆé€‚å†åˆ°è¿‡æ‹Ÿåˆçš„è¿‡ç¨‹ï¼Œè¿›è€Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„è®¾ç½®ï¼Œæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

### 1.3. åµŒå…¥å¼ç‰¹å¾é€‰æ‹©

* L1æ­£åˆ™åŒ–æœ‰åŠ©äºç”Ÿæˆä¸€ä¸ªç¨€ç–æƒå€¼çŸ©é˜µï¼Œè¿›è€Œå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ã€‚
* L2æ­£åˆ™åŒ–åœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­é€šå¸¸éƒ½å€¾å‘äºè®©æƒå€¼å°½å¯èƒ½å°ï¼Œæœ€åæ„é€ ä¸€ä¸ªæ‰€æœ‰å‚æ•°éƒ½æ¯”è¾ƒå°çš„æ¨¡å‹ã€‚å› ä¸ºä¸€èˆ¬è®¤ä¸ºå‚æ•°å€¼å°çš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œèƒ½é€‚åº”ä¸åŒçš„æ•°æ®é›†ï¼Œä¹Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚
* Lassoå›å½’ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œå²­å›å½’ä½¿ç”¨L1æ­£åˆ™åŒ–ã€‚
* å†³ç­–æ ‘é€šè¿‡ä¿¡æ¯ç†µæˆ–GINIæŒ‡æ•°é€‰æ‹©åˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©çš„åˆ†è£‚ç‰¹å¾ä¹Ÿæ›´åŠ é‡è¦ï¼Œè¿™åŒæ ·æ˜¯ä¸€ç§ç‰¹å¾é€‰æ‹©çš„æ–¹æ³•ã€‚

### 1.4. æ¨¡å‹è°ƒå‚

#### è´ªå¿ƒè°ƒå‚æ–¹æ³•

* æ‰€è°“è´ªå¿ƒç®—æ³•æ˜¯æŒ‡ï¼Œåœ¨å¯¹é—®é¢˜æ±‚è§£æ—¶ï¼Œæ€»æ˜¯åšå‡ºåœ¨å½“å‰çœ‹æ¥æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ä»æ•´ä½“æœ€ä¼˜ä¸ŠåŠ ä»¥è€ƒè™‘ï¼Œå®ƒæ‰€åšå‡ºçš„ä»…ä»…æ˜¯åœ¨æŸç§æ„ä¹‰ä¸Šçš„å±€éƒ¨æœ€ä¼˜è§£ã€‚é€‚ç”¨çš„å‰ææ˜¯ï¼šå±€éƒ¨æœ€ä¼˜ç­–ç•¥èƒ½å¯¼è‡´äº§ç”Ÿå…¨å±€æœ€ä¼˜è§£ã€‚
* åŸºæœ¬æ€è·¯

1) å»ºç«‹æ•°å­¦æ¨¡å‹æ¥æè¿°é—®é¢˜

2) æŠŠæ±‚è§£çš„é—®é¢˜åˆ†æˆè‹¥å¹²ä¸ªå­é—®é¢˜

3) å¯¹æ¯ä¸ªå­é—®é¢˜æ±‚è§£ï¼Œå¾—åˆ°å­é—®é¢˜çš„å±€éƒ¨æœ€ä¼˜è§£

4) æŠŠå­é—®é¢˜çš„è§£å±€éƒ¨æœ€ä¼˜è§£åˆæˆåŸæ¥é—®é¢˜çš„ä¸€ä¸ªè§£

#### ç½‘æ ¼è°ƒå‚æ–¹æ³•

* é€šè¿‡å¾ªç¯éå†ï¼Œå°è¯•æ¯ä¸€ç§å‚æ•°ç»„åˆï¼Œè¿”å›æœ€å¥½çš„å¾—åˆ†å€¼çš„å‚æ•°ç»„åˆã€‚
* GridSearchCVèƒ½å¤Ÿä½¿æˆ‘ä»¬æ‰¾åˆ°èŒƒå›´å†…æœ€ä¼˜çš„å‚æ•°ï¼Œparam_gridå‚æ•°è¶Šå¤šï¼Œç»„åˆè¶Šå¤šï¼Œè®¡ç®—çš„æ—¶é—´ä¹Ÿéœ€è¦è¶Šå¤šï¼ŒGridSearchCVä½¿ç”¨äºå°æ•°æ®é›†ã€‚

#### è´å¶æ–¯è°ƒå‚æ–¹æ³•

* è´å¶æ–¯ä¼˜åŒ–é€šè¿‡åŸºäºç›®æ ‡å‡½æ•°çš„è¿‡å»è¯„ä¼°ç»“æœå»ºç«‹æ›¿ä»£å‡½æ•°ï¼ˆæ¦‚ç‡æ¨¡å‹ï¼‰ï¼Œæ¥æ‰¾åˆ°æœ€å°åŒ–ç›®æ ‡å‡½æ•°çš„å€¼ã€‚è´å¶æ–¯æ–¹æ³•ä¸éšæœºæˆ–ç½‘æ ¼æœç´¢çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒåœ¨å°è¯•ä¸‹ä¸€ç»„è¶…å‚æ•°æ—¶ï¼Œä¼šå‚è€ƒä¹‹å‰çš„è¯„ä¼°ç»“æœï¼Œå› æ­¤å¯ä»¥çœå»å¾ˆå¤šæ— ç”¨åŠŸã€‚
* åŸºæœ¬æ€è·¯

1) ç›®æ ‡å‡½æ•°ï¼šæˆ‘ä»¬æƒ³è¦æœ€å°åŒ–çš„å†…å®¹ï¼Œåœ¨è¿™é‡Œï¼Œç›®æ ‡å‡½æ•°æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹ä½¿ç”¨è¯¥ç»„è¶…å‚æ•°åœ¨éªŒè¯é›†ä¸Šçš„æŸå¤±ã€‚

2) åŸŸç©ºé—´ï¼šè¦æœç´¢çš„è¶…å‚æ•°çš„å–å€¼èŒƒå›´

3) ä¼˜åŒ–ç®—æ³•ï¼šæ„é€ æ›¿ä»£å‡½æ•°å¹¶é€‰æ‹©ä¸‹ä¸€ä¸ªè¶…å‚æ•°å€¼è¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚

4) ç»“æœå†å²è®°å½•ï¼šæ¥è‡ªç›®æ ‡å‡½æ•°è¯„ä¼°çš„å­˜å‚¨ç»“æœï¼ŒåŒ…æ‹¬è¶…å‚æ•°å’ŒéªŒè¯é›†ä¸Šçš„æŸå¤±ã€‚

## äºŒã€ä»£ç å®æˆ˜

### 2.1 è¯»å–æ•°æ®

```python
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
```

reduce_mem_usage å‡½æ•°é€šè¿‡è°ƒæ•´æ•°æ®ç±»å‹ï¼Œå¸®åŠ©æˆ‘ä»¬å‡å°‘æ•°æ®åœ¨å†…å­˜ä¸­å ç”¨çš„ç©ºé—´


```python
def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() 
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() 
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return df
```


```python
sample_feature = reduce_mem_usage(pd.read_csv('data_for_tree.csv'))
```

    Memory usage of dataframe is 60507328.00 MB
    Memory usage after optimization is: 15724107.00 MB
    Decreased by 74.0%
    


```python
continuous_feature_names = [x for x in sample_feature.columns if x not in ['price','brand','model','brand']]
```

### 2.2 çº¿æ€§å›å½’ & äº”æŠ˜äº¤å‰éªŒè¯ & æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡æƒ…å†µ


```python
sample_feature = sample_feature.dropna().replace('-', 0).reset_index(drop=True)
sample_feature['notRepairedDamage'] = sample_feature['notRepairedDamage'].astype(np.float32)
train = sample_feature[continuous_feature_names + ['price']]

train_X = train[continuous_feature_names]
train_y = train['price']
```

#### 2.2 - 1 ç®€å•å»ºæ¨¡


```python
from sklearn.linear_model import LinearRegression
```


```python
model = LinearRegression(normalize=True)
```


```python
model = model.fit(train_X, train_y)
```

æŸ¥çœ‹è®­ç»ƒçš„çº¿æ€§å›å½’æ¨¡å‹çš„æˆªè·ï¼ˆinterceptï¼‰ä¸æƒé‡(coef)


```python
'intercept:'+ str(model.intercept_)

sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)
```

```python
from matplotlib import pyplot as plt
subsample_index = np.random.randint(low=0, high=len(train_y), size=50)
```

ç»˜åˆ¶ç‰¹å¾v_9çš„å€¼ä¸æ ‡ç­¾çš„æ•£ç‚¹å›¾ï¼Œå›¾ç‰‡å‘ç°æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆè“è‰²ç‚¹ï¼‰ä¸çœŸå®æ ‡ç­¾ï¼ˆé»‘è‰²ç‚¹ï¼‰çš„åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œä¸”éƒ¨åˆ†é¢„æµ‹å€¼å‡ºç°äº†å°äº0çš„æƒ…å†µï¼Œè¯´æ˜æˆ‘ä»¬çš„æ¨¡å‹å­˜åœ¨ä¸€äº›é—®é¢˜


```python
plt.scatter(train_X['v_9'][subsample_index], train_y[subsample_index], color='black')
plt.scatter(train_X['v_9'][subsample_index], model.predict(train_X.loc[subsample_index]), color='blue')
plt.xlabel('v_9')
plt.ylabel('price')
plt.legend(['True Price','Predicted Price'],loc='upper right')
print('The predicted price is obvious different from true price')
plt.show()
```

![output_22_1](https://img-blog.csdnimg.cn/20200321231804889.png)



é€šè¿‡ä½œå›¾æˆ‘ä»¬å‘ç°æ•°æ®çš„æ ‡ç­¾ï¼ˆpriceï¼‰å‘ˆç°é•¿å°¾åˆ†å¸ƒï¼Œä¸åˆ©äºæˆ‘ä»¬çš„å»ºæ¨¡é¢„æµ‹ã€‚åŸå› æ˜¯å¾ˆå¤šæ¨¡å‹éƒ½å‡è®¾æ•°æ®è¯¯å·®é¡¹ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œè€Œé•¿å°¾åˆ†å¸ƒçš„æ•°æ®è¿èƒŒäº†è¿™ä¸€å‡è®¾ã€‚

```python
import seaborn as sns
print('It is clear to see the price shows a typical exponential distribution')
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y)
plt.subplot(1,2,2)
sns.distplot(train_y[train_y < np.quantile(train_y, 0.9)])
```

![output_24_2](https://img-blog.csdnimg.cn/20200321231820197.png)


åœ¨è¿™é‡Œæˆ‘ä»¬å¯¹æ ‡ç­¾è¿›è¡Œäº† $log(x+1)$ å˜æ¢ï¼Œä½¿æ ‡ç­¾è´´è¿‘äºæ­£æ€åˆ†å¸ƒ


```python
train_y_ln = np.log(train_y + 1)
```


```python
import seaborn as sns
print('The transformed price seems like normal distribution')
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(train_y_ln)
plt.subplot(1,2,2)
sns.distplot(train_y_ln[train_y_ln < np.quantile(train_y_ln, 0.9)])
```

![output_27_2](https://img-blog.csdnimg.cn/20200321231840673.png)



```python
model = model.fit(train_X, train_y_ln)
```

å†æ¬¡è¿›è¡Œå¯è§†åŒ–ï¼Œå‘ç°é¢„æµ‹ç»“æœä¸çœŸå®å€¼è¾ƒä¸ºæ¥è¿‘ï¼Œä¸”æœªå‡ºç°å¼‚å¸¸çŠ¶å†µ


```python
plt.scatter(train_X['v_9'][subsample_index], train_y[subsample_index], color='black')
plt.scatter(train_X['v_9'][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color='blue')
plt.xlabel('v_9')
plt.ylabel('price')
plt.legend(['True Price','Predicted Price'],loc='upper right')
print('The predicted price seems normal after np.log transforming')
plt.show()
```

![output_30_1](https://img-blog.csdnimg.cn/20200321231902283.png)


#### 2.2 - 2 äº”æŠ˜äº¤å‰éªŒè¯

> åœ¨ä½¿ç”¨è®­ç»ƒé›†å¯¹å‚æ•°è¿›è¡Œè®­ç»ƒçš„æ—¶å€™ï¼Œç»å¸¸ä¼šå‘ç°äººä»¬é€šå¸¸ä¼šå°†ä¸€æ•´ä¸ªè®­ç»ƒé›†åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼ˆæ¯”å¦‚mnistæ‰‹å†™è®­ç»ƒé›†ï¼‰ã€‚ä¸€èˆ¬åˆ†ä¸ºï¼šè®­ç»ƒé›†ï¼ˆtrain_setï¼‰ï¼Œè¯„ä¼°é›†ï¼ˆvalid_setï¼‰ï¼Œæµ‹è¯•é›†ï¼ˆtest_setï¼‰è¿™ä¸‰ä¸ªéƒ¨åˆ†ã€‚è¿™å…¶å®æ˜¯ä¸ºäº†ä¿è¯è®­ç»ƒæ•ˆæœè€Œç‰¹æ„è®¾ç½®çš„ã€‚å…¶ä¸­æµ‹è¯•é›†å¾ˆå¥½ç†è§£ï¼Œå…¶å®å°±æ˜¯å®Œå…¨ä¸å‚ä¸è®­ç»ƒçš„æ•°æ®ï¼Œä»…ä»…ç”¨æ¥è§‚æµ‹æµ‹è¯•æ•ˆæœçš„æ•°æ®ã€‚è€Œè®­ç»ƒé›†å’Œè¯„ä¼°é›†åˆ™ç‰µæ¶‰åˆ°ä¸‹é¢çš„çŸ¥è¯†äº†ã€‚

>å› ä¸ºåœ¨å®é™…çš„è®­ç»ƒä¸­ï¼Œè®­ç»ƒçš„ç»“æœå¯¹äºè®­ç»ƒé›†çš„æ‹Ÿåˆç¨‹åº¦é€šå¸¸è¿˜æ˜¯æŒºå¥½çš„ï¼ˆåˆå§‹æ¡ä»¶æ•æ„Ÿï¼‰ï¼Œä½†æ˜¯å¯¹äºè®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦é€šå¸¸å°±ä¸é‚£ä¹ˆä»¤äººæ»¡æ„äº†ã€‚å› æ­¤æˆ‘ä»¬é€šå¸¸å¹¶ä¸ä¼šæŠŠæ‰€æœ‰çš„æ•°æ®é›†éƒ½æ‹¿æ¥è®­ç»ƒï¼Œè€Œæ˜¯åˆ†å‡ºä¸€éƒ¨åˆ†æ¥ï¼ˆè¿™ä¸€éƒ¨åˆ†ä¸å‚åŠ è®­ç»ƒï¼‰å¯¹è®­ç»ƒé›†ç”Ÿæˆçš„å‚æ•°è¿›è¡Œæµ‹è¯•ï¼Œç›¸å¯¹å®¢è§‚çš„åˆ¤æ–­è¿™äº›å‚æ•°å¯¹è®­ç»ƒé›†ä¹‹å¤–çš„æ•°æ®çš„ç¬¦åˆç¨‹åº¦ã€‚è¿™ç§æ€æƒ³å°±ç§°ä¸ºäº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰


```python
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error,  make_scorer

def log_transfer(func):
    def wrapper(y, yhat):
        result = func(np.log(y), np.nan_to_num(np.log(yhat)))
        return result
    return wrapper
```


```python
scores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5, scoring=make_scorer(log_transfer(mean_absolute_error)))
```

ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¯¹æœªå¤„ç†æ ‡ç­¾çš„ç‰¹å¾æ•°æ®è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆError 1.36ï¼‰


```python
print('AVG:', np.mean(scores))
```

    AVG: 1.3641908155886227
    

ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¯¹å¤„ç†è¿‡æ ‡ç­¾çš„ç‰¹å¾æ•°æ®è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆError 0.19ï¼‰


```python
scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error))
```

```python
print('AVG:', np.mean(scores))
```

    AVG: 0.19382863663604424
    


```python
scores = pd.DataFrame(scores.reshape(1,-1))
scores.columns = ['cv' + str(x) for x in range(1, 6)]
scores.index = ['MAE']
scores
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cv1</th>
      <th>cv2</th>
      <th>cv3</th>
      <th>cv4</th>
      <th>cv5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MAE</th>
      <td>0.191642</td>
      <td>0.194986</td>
      <td>0.192737</td>
      <td>0.195329</td>
      <td>0.19445</td>
    </tr>
  </tbody>
</table>
</div>



#### 2.2 - 3 æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡æƒ…å†µ

ä½†åœ¨äº‹å®ä¸Šï¼Œç”±äºæˆ‘ä»¬å¹¶ä¸å…·æœ‰é¢„çŸ¥æœªæ¥çš„èƒ½åŠ›ï¼Œäº”æŠ˜äº¤å‰éªŒè¯åœ¨æŸäº›ä¸æ—¶é—´ç›¸å…³çš„æ•°æ®é›†ä¸Šåè€Œåæ˜ äº†ä¸çœŸå®çš„æƒ…å†µã€‚é€šè¿‡2018å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼é¢„æµ‹2017å¹´çš„äºŒæ‰‹è½¦ä»·æ ¼ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å¯ä»¥é‡‡ç”¨æ—¶é—´é¡ºåºå¯¹æ•°æ®é›†è¿›è¡Œåˆ†éš”ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰ç”¨é å‰æ—¶é—´çš„4/5æ ·æœ¬å½“ä½œè®­ç»ƒé›†ï¼Œé åæ—¶é—´çš„1/5å½“ä½œéªŒè¯é›†ï¼Œæœ€ç»ˆç»“æœä¸äº”æŠ˜äº¤å‰éªŒè¯å·®è·ä¸å¤§

```python
import datetime

sample_feature = sample_feature.reset_index(drop=True)

split_point = len(sample_feature) // 5 * 4

train = sample_feature.loc[:split_point].dropna()
val = sample_feature.loc[split_point:].dropna()

train_X = train[continuous_feature_names]
train_y_ln = np.log(train['price'] + 1)
val_X = val[continuous_feature_names]
val_y_ln = np.log(val['price'] + 1)
```

```python
model = model.fit(train_X, train_y_ln)

mean_absolute_error(val_y_ln, model.predict(val_X))
```

    0.19443858353490887

#### 2.2 - 4 ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ä¸éªŒè¯æ›²çº¿

```python
from sklearn.model_selection import learning_curve, validation_curve

def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_size=np.linspace(.1, 1.0, 5 )):  
    plt.figure()  
    plt.title(title)  
    if ylim is not None:  
        plt.ylim(*ylim)  
    plt.xlabel('Training example')  
    plt.ylabel('score')  
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  
    train_scores_mean = np.mean(train_scores, axis=1)  
    train_scores_std = np.std(train_scores, axis=1)  
    test_scores_mean = np.mean(test_scores, axis=1)  
    test_scores_std = np.std(test_scores, axis=1)  
    plt.grid()#åŒºåŸŸ  
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  
                     train_scores_mean + train_scores_std, alpha=0.1,  
                     color="r")  
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  
                     test_scores_mean + test_scores_std, alpha=0.1,  
                     color="g")  
    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',  
             label="Training score")  
    plt.plot(train_sizes, test_scores_mean,'o-',color="g",  
             label="Cross-validation score")  
    plt.legend(loc="best")  
    return plt  

plot_learning_curve(LinearRegression(), 'Liner_model', train_X[:1000], train_y_ln[:1000], ylim=(0.0, 0.5), cv=5, n_jobs=1)  
```

![54-1](https://img-blog.csdnimg.cn/20200321231918241.png)


#### 2.3 å¤šç§æ¨¡å‹å¯¹æ¯”


```python
train = sample_feature[continuous_feature_names + ['price']].dropna()

train_X = train[continuous_feature_names]
train_y = train['price']
train_y_ln = np.log(train_y + 1)
```

#### 2.3 - 1 çº¿æ€§æ¨¡å‹ & åµŒå…¥å¼ç‰¹å¾é€‰æ‹©

```python
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

models = [LinearRegression(),
          Ridge(),
          Lasso()]

result = dict()
for model in models:
    model_name = str(model).split('(')[0]
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))
    result[model_name] = scores
    print(model_name + ' is finished')
```

å¯¹ä¸‰ç§æ–¹æ³•çš„æ•ˆæœå¯¹æ¯”


```python
result = pd.DataFrame(result)
result.index = ['cv' + str(x) for x in range(1, 6)]
result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>Ridge</th>
      <th>Lasso</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cv1</th>
      <td>0.191642</td>
      <td>0.195665</td>
      <td>0.382708</td>
    </tr>
    <tr>
      <th>cv2</th>
      <td>0.194986</td>
      <td>0.198841</td>
      <td>0.383916</td>
    </tr>
    <tr>
      <th>cv3</th>
      <td>0.192737</td>
      <td>0.196629</td>
      <td>0.380754</td>
    </tr>
    <tr>
      <th>cv4</th>
      <td>0.195329</td>
      <td>0.199255</td>
      <td>0.385683</td>
    </tr>
    <tr>
      <th>cv5</th>
      <td>0.194450</td>
      <td>0.198173</td>
      <td>0.383555</td>
    </tr>
  </tbody>
</table>
</div>




```python
model = LinearRegression().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:23.515984499017883

![output_65_2](https://img-blog.csdnimg.cn/20200321231945959.png)


```python
model = Ridge().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:5.901527844424091

![output_67_2](https://img-blog.csdnimg.cn/20200321232121957.png)


```python
model = Lasso().fit(train_X, train_y_ln)
print('intercept:'+ str(model.intercept_))
sns.barplot(abs(model.coef_), continuous_feature_names)
```

    intercept:8.674427764003347

![output_69_2](https://img-blog.csdnimg.cn/202003212321463.png)

#### 2.3 - 2 éçº¿æ€§æ¨¡å‹

```python
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from xgboost.sklearn import XGBRegressor
from lightgbm.sklearn import LGBMRegressor

models = [LinearRegression(),
          DecisionTreeRegressor(),
          RandomForestRegressor(),
          GradientBoostingRegressor(),
          MLPRegressor(solver='lbfgs', max_iter=100), 
          XGBRegressor(n_estimators = 100, objective='reg:squarederror'), 
          LGBMRegressor(n_estimators = 100)]

result = dict()
for model in models:
    model_name = str(model).split('(')[0]
    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error))
    result[model_name] = scores
    print(model_name + ' is finished')
```

```python
result = pd.DataFrame(result)
result.index = ['cv' + str(x) for x in range(1, 6)]
result
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>DecisionTreeRegressor</th>
      <th>RandomForestRegressor</th>
      <th>GradientBoostingRegressor</th>
      <th>MLPRegressor</th>
      <th>XGBRegressor</th>
      <th>LGBMRegressor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cv1</th>
      <td>0.191642</td>
      <td>0.184566</td>
      <td>0.136266</td>
      <td>0.168626</td>
      <td>124.299426</td>
      <td>0.168698</td>
      <td>0.141159</td>
    </tr>
    <tr>
      <th>cv2</th>
      <td>0.194986</td>
      <td>0.187029</td>
      <td>0.139693</td>
      <td>0.171905</td>
      <td>257.886236</td>
      <td>0.172258</td>
      <td>0.143363</td>
    </tr>
    <tr>
      <th>cv3</th>
      <td>0.192737</td>
      <td>0.184839</td>
      <td>0.136871</td>
      <td>0.169553</td>
      <td>236.829589</td>
      <td>0.168604</td>
      <td>0.142137</td>
    </tr>
    <tr>
      <th>cv4</th>
      <td>0.195329</td>
      <td>0.182605</td>
      <td>0.138689</td>
      <td>0.172299</td>
      <td>130.197264</td>
      <td>0.172474</td>
      <td>0.143461</td>
    </tr>
    <tr>
      <th>cv5</th>
      <td>0.194450</td>
      <td>0.186626</td>
      <td>0.137420</td>
      <td>0.171206</td>
      <td>268.090236</td>
      <td>0.170898</td>
      <td>0.141921</td>
    </tr>
  </tbody>
</table>
</div>



å¯ä»¥çœ‹åˆ°éšæœºæ£®æ—æ¨¡å‹åœ¨æ¯ä¸€ä¸ªfoldä¸­å‡å–å¾—äº†æ›´å¥½çš„æ•ˆæœ

#### 2.4  æ¨¡å‹è°ƒå‚

```python
## LGBçš„å‚æ•°é›†åˆï¼š

objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']

num_leaves = [3,5,10,15,20,40, 55]
max_depth = [3,5,10,15,20,40, 55]
bagging_fraction = []
feature_fraction = []
drop_rate = []
```

#### 2.4 - 1 è´ªå¿ƒè°ƒå‚


```python
best_obj = dict()
for obj in objective:
    model = LGBMRegressor(objective=obj)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_obj[obj] = score
    
best_leaves = dict()
for leaves in num_leaves:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_leaves[leaves] = score
    
best_depth = dict()
for depth in max_depth:
    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],
                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],
                          max_depth=depth)
    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
    best_depth[depth] = score
```


```python
sns.lineplot(x=['0_initial','1_turning_obj','2_turning_leaves','3_turning_depth'], y=[0.143 ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())])
```

![83-1](https://img-blog.csdnimg.cn/20200321232159934.png)

#### 2.4 - 2 Grid Search è°ƒå‚


```python
from sklearn.model_selection import GridSearchCV

parameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}
model = LGBMRegressor()
clf = GridSearchCV(model, parameters, cv=5)
clf = clf.fit(train_X, train_y)

clf.best_params_
```




    {'max_depth': 15, 'num_leaves': 55, 'objective': 'regression'}




```python
model = LGBMRegressor(objective='regression',
                          num_leaves=55,
                          max_depth=15)
```


```python
np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))
```

    0.13626164479243302

#### 2.4 - 3 è´å¶æ–¯è°ƒå‚

```python
from bayes_opt import BayesianOptimization

def rf_cv(num_leaves, max_depth, subsample, min_child_samples):
    val = cross_val_score(
        LGBMRegressor(objective = 'regression_l1',
            num_leaves=int(num_leaves),
            max_depth=int(max_depth),
            subsample = subsample,
            min_child_samples = int(min_child_samples)
        ),
        X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)
    ).mean()
    return 1 - val

rf_bo = BayesianOptimization(
    rf_cv,
    {
    'num_leaves': (2, 100),
    'max_depth': (2, 100),
    'subsample': (0.1, 1),
    'min_child_samples' : (2, 100)
    }
)

rf_bo.maximize()
```

    |   iter    |  target   | max_depth | min_ch... | num_le... | subsample |
    -------------------------------------------------------------------------
    | [0m 1       [0m | [0m 0.8649  [0m | [0m 89.57   [0m | [0m 47.3    [0m | [0m 55.13   [0m | [0m 0.1792  [0m |
    | [0m 2       [0m | [0m 0.8477  [0m | [0m 99.86   [0m | [0m 60.91   [0m | [0m 15.35   [0m | [0m 0.4716  [0m |
    | [95m 3       [0m | [95m 0.8698  [0m | [95m 81.74   [0m | [95m 83.32   [0m | [95m 92.59   [0m | [95m 0.9559  [0m |
    | [0m 4       [0m | [0m 0.8627  [0m | [0m 90.2    [0m | [0m 8.754   [0m | [0m 43.34   [0m | [0m 0.7772  [0m |
    | [0m 5       [0m | [0m 0.8115  [0m | [0m 10.07   [0m | [0m 86.15   [0m | [0m 4.109   [0m | [0m 0.3416  [0m |
    | [95m 6       [0m | [95m 0.8701  [0m | [95m 99.15   [0m | [95m 9.158   [0m | [95m 99.47   [0m | [95m 0.494   [0m |
    | [0m 7       [0m | [0m 0.806   [0m | [0m 2.166   [0m | [0m 2.416   [0m | [0m 97.7    [0m | [0m 0.224   [0m |
    | [0m 8       [0m | [0m 0.8701  [0m | [0m 98.57   [0m | [0m 97.67   [0m | [0m 99.87   [0m | [0m 0.3703  [0m |
    | [95m 9       [0m | [95m 0.8703  [0m | [95m 99.87   [0m | [95m 43.03   [0m | [95m 99.72   [0m | [95m 0.9749  [0m |
    | [0m 10      [0m | [0m 0.869   [0m | [0m 10.31   [0m | [0m 99.63   [0m | [0m 99.34   [0m | [0m 0.2517  [0m |
    | [95m 11      [0m | [95m 0.8703  [0m | [95m 52.27   [0m | [95m 99.56   [0m | [95m 98.97   [0m | [95m 0.9641  [0m |
    | [0m 12      [0m | [0m 0.8669  [0m | [0m 99.89   [0m | [0m 8.846   [0m | [0m 66.49   [0m | [0m 0.1437  [0m |
    | [0m 13      [0m | [0m 0.8702  [0m | [0m 68.13   [0m | [0m 75.28   [0m | [0m 98.71   [0m | [0m 0.153   [0m |
    | [0m 14      [0m | [0m 0.8695  [0m | [0m 84.13   [0m | [0m 86.48   [0m | [0m 91.9    [0m | [0m 0.7949  [0m |
    | [0m 15      [0m | [0m 0.8702  [0m | [0m 98.09   [0m | [0m 59.2    [0m | [0m 99.65   [0m | [0m 0.3275  [0m |
    | [0m 16      [0m | [0m 0.87    [0m | [0m 68.97   [0m | [0m 98.62   [0m | [0m 98.93   [0m | [0m 0.2221  [0m |
    | [0m 17      [0m | [0m 0.8702  [0m | [0m 99.85   [0m | [0m 63.74   [0m | [0m 99.63   [0m | [0m 0.4137  [0m |
    | [0m 18      [0m | [0m 0.8703  [0m | [0m 45.87   [0m | [0m 99.05   [0m | [0m 99.89   [0m | [0m 0.3238  [0m |
    | [0m 19      [0m | [0m 0.8702  [0m | [0m 79.65   [0m | [0m 46.91   [0m | [0m 98.61   [0m | [0m 0.8999  [0m |
    | [0m 20      [0m | [0m 0.8702  [0m | [0m 99.25   [0m | [0m 36.73   [0m | [0m 99.05   [0m | [0m 0.1262  [0m |
    | [0m 21      [0m | [0m 0.8702  [0m | [0m 85.51   [0m | [0m 85.34   [0m | [0m 99.77   [0m | [0m 0.8917  [0m |
    | [0m 22      [0m | [0m 0.8696  [0m | [0m 99.99   [0m | [0m 38.51   [0m | [0m 89.13   [0m | [0m 0.9884  [0m |
    | [0m 23      [0m | [0m 0.8701  [0m | [0m 63.29   [0m | [0m 97.93   [0m | [0m 99.94   [0m | [0m 0.9585  [0m |
    | [0m 24      [0m | [0m 0.8702  [0m | [0m 93.04   [0m | [0m 71.42   [0m | [0m 99.94   [0m | [0m 0.9646  [0m |
    | [0m 25      [0m | [0m 0.8701  [0m | [0m 99.73   [0m | [0m 16.21   [0m | [0m 99.38   [0m | [0m 0.9778  [0m |
    | [0m 26      [0m | [0m 0.87    [0m | [0m 86.28   [0m | [0m 58.1    [0m | [0m 99.47   [0m | [0m 0.107   [0m |
    | [0m 27      [0m | [0m 0.8703  [0m | [0m 47.28   [0m | [0m 99.83   [0m | [0m 99.65   [0m | [0m 0.4674  [0m |
    | [0m 28      [0m | [0m 0.8703  [0m | [0m 68.29   [0m | [0m 99.51   [0m | [0m 99.4    [0m | [0m 0.2757  [0m |
    | [0m 29      [0m | [0m 0.8701  [0m | [0m 76.49   [0m | [0m 73.41   [0m | [0m 99.86   [0m | [0m 0.9394  [0m |
    | [0m 30      [0m | [0m 0.8695  [0m | [0m 37.27   [0m | [0m 99.87   [0m | [0m 89.87   [0m | [0m 0.7588  [0m |
    =========================================================================
    


```python
1 - rf_bo.max['target']
```

    0.1296693644053145